{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para limpiar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def limpia_texto(texto):\n",
    "\treturn ' '.join(\n",
    "\t\t[palabra.lower() for palabra in \n",
    "\t\t\tre.sub(\n",
    "\t\t\t\t\"(@[A-Za-z0-9]+)|([A-Za-zá-ú]*[0-9]+[A-Za-zá-ú]*)|([^A-Za-z \\tá-ú])|(\\w+:\\/\\/\\S+)\", \n",
    "\t\t\t\t\" \", \n",
    "\t\t\t\ttexto\n",
    "\t\t\t).split()]\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.csv', 'r') as f:\n",
    "\traw = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "\n",
    "\tdata = []\n",
    "\tfor row in raw:\n",
    "\t\tdata.append([\n",
    "\t\t\trow[0],\n",
    "\t\t\trow[1],\n",
    "\t\t\tlimpia_texto(row[0])\n",
    "\t\t])\n",
    "\n",
    "tweets_limpios = np.asarray(data)[:,2]\n",
    "Y = np.asarray(data)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14312,)\n",
      "(14312,)\n"
     ]
    }
   ],
   "source": [
    "print(tweets_limpios.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 22286 palabras\n"
     ]
    }
   ],
   "source": [
    "bolsa_de_palabras = sorted(set(' '.join(tweets_limpios).split()))\n",
    "\n",
    "print(\"Hay {} palabras\".format(len(bolsa_de_palabras)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def Lemmatizador(texto, minWordLen=4):\n",
    "\n",
    "\ttokens = texto.split()\n",
    "\n",
    "\toutput = []\n",
    "\tfor palabra in tokens:\n",
    "\t\tcorregida = palabra\n",
    "\n",
    "\t\tregla1 = r'((i[eé]ndo|[aá]ndo|[aáeéií]r|[^u]yendo)(sel[ao]s?|l[aeo]s?|nos|se|me))'\n",
    "\t\tstep1 = re.search(regla1, corregida)\n",
    "\t\tif step1:\n",
    "\t\t    if (len(palabra)-len(step1.group(1))) >= minWordLen:\n",
    "\t\t        corregida = corregida[:-len(step1.group(1))]\n",
    "\t\t    elif (len(palabra)-len(step1.group(3))) >= minWordLen:\n",
    "\t\t        corregida = corregida[:-len(step1.group(3))]\n",
    "\n",
    "\t\tregla2 = {\n",
    "\t\t  '(anzas?|ic[oa]s?|ismos?|[ai]bles?|istas?|os[oa]s?|[ai]mientos?)$' : '',\n",
    "\t\t  '((ic)?(adora?|ación|ador[ae]s|aciones|antes?|ancias?))$' : '',\n",
    "\t\t  '(log[íi]as?)$' : 'log',\n",
    "\t\t  '(ución|uciones)$' : 'u',\n",
    "\t\t  '(encias?)$' : 'ente',\n",
    "\t\t  '((os|ic|ad|(at)?iv)amente)$' : '',\n",
    "\t\t  '(amente)$' : '',\n",
    "\t\t  '((ante|[ai]ble)?mente)$' : '',\n",
    "\t\t  '((abil|ic|iv)?idad(es)?)$' : '',\n",
    "\t\t  '((at)?iv[ao]s?)$' : '',\n",
    "\t\t  '(ad[ao])$' : '',\n",
    "\t\t  '(ando)$' : '',\n",
    "\t\t  '(aci[óo]n)$' : '',\n",
    "\t\t  '(es)$' : ''\n",
    "\t\t}\n",
    "\t\tfor key in regla2:\n",
    "\t\t    tmp = re.sub(key, regla2[key], corregida)\n",
    "\t\t    if tmp!=corregida and len(tmp)>=minWordLen:\n",
    "\t\t        corregida = tmp\n",
    "\n",
    "\t\tregla3 = {\n",
    "\t\t'(y[ae]n?|yeron|yendo|y[oó]|y[ae]s|yais|yamos)$',\n",
    "\t\t'(en|es|éis|emos)$',\n",
    "\t\t'(([aei]ría|ié(ra|se))mos)$',\n",
    "\t\t'(([aei]re|á[br]a|áse)mos)$',\n",
    "\t\t'([aei]ría[ns]|[aei]réis|ie((ra|se)[ns]|ron|ndo)|a[br]ais|aseis|íamos)$',\n",
    "\t\t'([aei](rá[ns]|ría)|a[bdr]as|id[ao]s|íais|([ai]m|ad)os|ie(se|ra)|[ai]ste|aban|ar[ao]n|ase[ns]|ando)$',\n",
    "\t\t'([aei]r[áé]|a[bdr]a|[ai]d[ao]|ía[ns]|áis|ase)$',\n",
    "\t\t'(í[as]|[aei]d|a[ns]|ió|[aei]r)$',\n",
    "\t\t'(os|a|o|á|í|ó)$',\n",
    "\t\t'(u?é|u?e)$',\n",
    "\t\t'(ual)$',\n",
    "\t\t'([áa]tic[oa]?)$'\n",
    "\t\t}\n",
    "\t\tfor pattern in regla3:\n",
    "\t\t    tmp = re.sub(pattern, '', corregida)\n",
    "\t\t    if tmp!=corregida and len(tmp)>=minWordLen:\n",
    "\t\t        corregida = tmp\n",
    "\n",
    "\t\toutput.append(corregida)\n",
    "\treturn ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 13108 raices\n"
     ]
    }
   ],
   "source": [
    "bolsa_de_raices = sorted(set(Lemmatizador(' '.join(bolsa_de_palabras)).split()))\n",
    "\n",
    "print(\"Hay {} raices\".format(len(bolsa_de_raices)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train vs test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10734,)\n",
      "(10734,)\n",
      "(3578,)\n",
      "(3578,)\n"
     ]
    }
   ],
   "source": [
    "num_test = int(0.25 * len(Y))\n",
    "\n",
    "tweets_limpios_test = tweets_limpios[-num_test:]\n",
    "Y_test = Y[-num_test:]\n",
    "\n",
    "tweets_limpios_train = tweets_limpios[:-num_test]\n",
    "\n",
    "Y_train = Y[:-num_test]\n",
    "\n",
    "\n",
    "print(tweets_limpios_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(tweets_limpios_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lemmatizado = [Lemmatizador(tweet) for tweet in tweets_limpios_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a las primer solicitud recibid se les enviar una especi de entr para acced al event merendolatuiter\n",
      "buen días twittercill falt d as para el estr yeswespainisdifferent en palm entrad en ole\n",
      "curr romer y su mujer carm tell en el para arrop a y pp\n",
      "venen\n",
      "me encant ir a bilba y encontr con quien le dice a su hijo germán es niet de uno de los fund del afhletic\n"
     ]
    }
   ],
   "source": [
    "for row in train_lemmatizado[10:15]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(input_, bolsa, invierte=False):\n",
    "\tif not invierte:\n",
    "\t\tonehot = np.zeros(len(bolsa))\n",
    "\t\tif input_ in bolsa:\n",
    "\t\t\tindice = bolsa.index(input_)\n",
    "\t\t\tonehot[indice] = 1\n",
    "\t\treturn onehot\n",
    "\telse:\n",
    "\t\tindice = np.flatnonzero(input_==1)\n",
    "\t\treturn None if len(indice)!=1 else bolsa[indice[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorizado = [one_hot(tweet, bolsa_de_raices) for tweet in train_lemmatizado]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a las primer solicitud recibid se les enviar una especi de entr para acced al event merendolatuiter\n"
     ]
    }
   ],
   "source": [
    "print(train_lemmatizado[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'primer'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lemmatizado[10].split()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-b72da18716c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_lemmatizado\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbolsa_de_raices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "one_hot(train_lemmatizado[10].split()[2], bolsa_de_raices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A las primeras 500 solicitudes recibidas se les enviara una especie de \"entrada\" para acceder al \"evento\".#merendolatuitera\n",
      "a las primer solicitud recibid se les enviar una especi de entr para acced al event merendolatuiter\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "¡¡¡buenos días twittercillos!! FALTAN 2 DÍAS PARA EL ESTRENO #yeswespainISDIFFERENT en PALMA!!! Entradas en http://t.co/Q9HzeyLP !!ole!\n",
      "buen días twittercill falt d as para el estr yeswespainisdifferent en palm entrad en ole\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Curro Romero y su mujer Carmen Tello en el #17CongresoPP para arropar a @marianorajoy y @javierarenas_pp http://t.co/L3m57MdG\n",
      "curr romer y su mujer carm tell en el para arrop a y pp\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "venenos\n",
      "venen\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Me encanta ir a Bilbao y encontrarme con quien le dice a su hijo: \"Germán es... nieto de uno de los fundadores del Afhletic\"\n",
      "me encant ir a bilba y encontr con quien le dice a su hijo germán es niet de uno de los fund del afhletic\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "train_onehot = []\n",
    "\n",
    "for i, tweet in enumerate(train_lemmatizado[10:15]):\n",
    "    \n",
    "    raices = [Lemmatizador(palabra) for palabra in tweet.split()]\n",
    "    \n",
    "    one_hots = [one_hot(raiz, bolsa_de_raices) for raiz in raices]\n",
    "\n",
    "    print(data[10+i][0])\n",
    "    print(tweet)\n",
    "    \n",
    "    train_onehot.append(one_hots)\n",
    "    \n",
    "    for one_hot_ in one_hots:\n",
    "        print(one_hot_)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "train_counts = []\n",
    "for row in train_onehot:\n",
    "    \n",
    "    counts = np.sum(row, axis=0)\n",
    "    \n",
    "    train_counts.append(counts)\n",
    "    \n",
    "    print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizar (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14312,)\n",
      "a las primeras solicitudes recibidas se les enviara una especie de entrada para acceder al evento merendolatuitera\n"
     ]
    }
   ],
   "source": [
    "print(tweets_limpios.shape)\n",
    "print(tweets_limpios[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=Lemmatizador)\n",
    "train_counts_2 = vectorizer.fit_transform(tweets_limpios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14312, 13080)\n"
     ]
    }
   ],
   "source": [
    "print(train_counts_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7968)\t1\n",
      "  (0, 4788)\t1\n",
      "  (0, 384)\t1\n",
      "  (0, 113)\t1\n",
      "  (0, 9055)\t1\n",
      "  (0, 4387)\t1\n",
      "  (0, 3060)\t1\n",
      "  (0, 4582)\t1\n",
      "  (0, 12413)\t1\n",
      "  (0, 4413)\t1\n",
      "  (0, 7273)\t1\n",
      "  (0, 10299)\t1\n",
      "  (0, 11411)\t1\n",
      "  (0, 9827)\t1\n",
      "  (0, 11056)\t1\n",
      "  (0, 7183)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_counts_2[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt','r') as f:\n",
    "\tstopwords = [line.rstrip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3),\n",
       "        preprocessor=<function Lemmatizador at 0x...enizer=None, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipe = Pipeline([\n",
    "\t('vect',CountVectorizer(ngram_range=(1,3), preprocessor=Lemmatizador)),  #, stop_words=stopwords\n",
    "\t#('tfidf',TfidfTransformer()),\n",
    "\t('clf',MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe.fit(tweets_limpios_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eficacia NB: 0.849357182783678\n"
     ]
    }
   ],
   "source": [
    "Y_pred = pipe.predict(tweets_limpios_test)\n",
    "\n",
    "\n",
    "print(\"Eficacia NB: {}\".format(np.mean(Y_pred==Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
